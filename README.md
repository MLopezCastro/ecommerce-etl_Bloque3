# ğŸ§ª ETL Pipeline â€“ Scalable Architecture with Python, Parquet & CLI

## ğŸ“ Overview

This project represents the third stage of a progressive ETL pipeline development. After building a functional local script and transforming it into a modular and testable architecture, this phase focuses on **scalability, format optimization, and execution control**.

The pipeline ingests raw transactional `.csv` data from an ecommerce context, applies cleaning, enrichment, and business validations, and exports ready-to-analyze datasets in **CSV or Parquet** format, supporting **Snappy compression** for performance.

---

## ğŸŒŸ Technical Objectives

- Build a **scalable and modular** local ETL pipeline ready for integration with tools like **Airflow**, **AWS Lambda**, or **GitHub Actions**.
- Adopt **Parquet**, a columnar data format widely used in data lakes and analytics systems, with **Snappy** compression for efficiency.
- Implement a **Command Line Interface (CLI)** using `argparse` to make execution dynamic and reproducible.
- Apply **professional best practices**: modular code, reusable functions, logging, and clear logic separation.

---

## âœ… Key Features & Deliverables

- **Modular ETL pipeline**: Scripts divided into clear layers for transformation, validation, export, and execution.
- **Scalable output format**: Export to **CSV or Parquet** (with Snappy compression).
- **Business logic enforcement**: Validation rules ensure data quality (e.g. no nulls, valid emails, allowed statuses).
- **Logging**: Timestamped `.log` file captures process duration, errors, and row counts.
- **Command-line interface (CLI)**: Run pipeline with full control over input/output and format selection.
- **Professional folder structure**: Clean separation for raw data, output, logs, and code.

---

## ğŸ§  Why Parquet + Snappy?

> Parquet is a **columnar storage format** optimized for big data analytics:
- Reads only needed columns â†’ faster queries.
- Supports advanced compression (Snappy, Gzip).
- Compatible with Athena, BigQuery, Spark, pandas.

> Snappy is a fast, lightweight compression format:
- Recommended for production environments.
- Used by Google, Facebook, and many open-source tools.
- Balances speed and compression size.

---

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**
- **pandas**
- **pyarrow** (Parquet support)
- **argparse** (CLI parser)
- **logging** (monitoring)

---

## ğŸ“ Project Structure

```
ecommerce-etl-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ orders.csv
â”‚   â”‚   â””â”€â”€ customers.csv
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ orders_enriched.snappy.parquet
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ etl.log
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ transform.py       # Data cleaning and enrichment
â”‚   â”œâ”€â”€ validate.py        # Business rules
â”‚   â”œâ”€â”€ export.py          # Output to CSV or Parquet
â”‚   â””â”€â”€ helpers.py         # (optional) utilities
â”œâ”€â”€ main.py                # CLI orchestrator
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run the Pipeline

1. Install dependencies:

```bash
pip install -r requirements.txt
```

2. Run pipeline with CLI:

```bash
python main.py   --orders data/raw/orders.csv   --customers data/raw/customers.csv   --output data/output/orders_enriched.parquet   --format parquet
```

3. Output will be logged in `logs/etl.log`.

---

## ğŸ§ª Example Validations

- `order_id` must be unique
- `quantity`, `unit_price` must be > 0
- `email` must be valid
- `status` must belong to accepted values
- `order_date` must not be in the future
- No nulls in critical fields (`country`, `email`)

---

## ğŸ“ Learning Outcomes

- Learned the **difference between row-based (CSV) and column-based (Parquet)** formats and when to use them.
- Gained experience building **reusable ETL components** ready for orchestration.
- Created a **CLI interface** to run flexible data pipelines.
- Practiced **production-grade logging** and error tracking.
- Set the foundation for **future cloud automation** (Airflow, AWS).

---

## ğŸ™Œ Acknowledgment

Special thanks to **Data Engineer Ian Saura**, whose clear guidance helped me execute this project successfully and think like a real data engineer.